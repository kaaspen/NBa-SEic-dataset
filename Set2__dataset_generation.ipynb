{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install bedtools"
      ],
      "metadata": {
        "id": "45-kf-adKYXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "PqWyS1Bg8aJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import umap.umap_ as umap\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hnpM14NOJOzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "L-GMT2HXLHTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Super_Enhancers/mm_project'"
      ],
      "metadata": {
        "id": "fxgv6S-J8os-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ['11_0077', '12_0118', '12_0449', '12_0450']"
      ],
      "metadata": {
        "id": "BTP8tYwniTTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting all enhancers data for each sample and concatenation into:**\n",
        "- Single SE dataset\n",
        "- Single TE dataset\n",
        "- Single dataset consisting of unioned SE and TE datasets\n"
      ],
      "metadata": {
        "id": "0T5Q3ZfxMHQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SE_table = pd.DataFrame()\n",
        "TE_table = pd.DataFrame()\n",
        "df_ste = pd.DataFrame()\n",
        "\n",
        "for sample in samples:\n",
        "  df_curr_se = pd.read_csv(f'{path}/Mouse_Lung_mm10/SE_{sample}_SE_mm10.bed', sep = '\\t')\n",
        "  df_curr_te = pd.read_csv(f'{path}/Mouse_Lung_mm10/SE_{sample}_TE_mm10.bed', sep = '\\t')\n",
        "\n",
        "  df_curr_se['region_length'] = df_curr_se.se_end - df_curr_se.se_start\n",
        "  df_curr_te['region_length'] = df_curr_te.TE_end - df_curr_te.TE_start\n",
        "\n",
        "  # handling corrupted data in TE dataset\n",
        "  df_curr_te = df_curr_te[~df_curr_te.cell_id.isnull()]\n",
        "  df_curr_te = df_curr_te[~df_curr_te.TE_cas_value.isnull()]\n",
        "  df_curr_te['TE_con_value'] = df_curr_te.TE_con_value.astype('float')\n",
        "  df_curr_te = df_curr_te[df_curr_te.TE_con_value < df_curr_te.TE_cas_value]\n",
        "  df_curr_te = df_curr_te.drop_duplicates()\n",
        "\n",
        "  df_curr_te['avg_rpm_diff'] = df_curr_te.TE_cas_value - df_curr_te.TE_con_value\n",
        "  df_curr_se['avg_rpm_diff'] = df_curr_se.se_cas_value - df_curr_se.se_con_value\n",
        "\n",
        "  df_ste_curr = pd.concat([df_curr_se[['cell_id', 'se_id','se_chr','se_start','se_end',\n",
        "                                       'se_rank', 'region_length', 'avg_rpm_diff']].rename(columns = {'se_id' : 'ste_id',\n",
        "                                                                                                      'se_rank': 'ste_rank',\n",
        "                                                                                                      'se_chr' : 'ste_chr',\n",
        "                                                                                                      'se_start' : 'ste_start',\n",
        "                                                                                                      'se_end' : 'ste_end'}),\n",
        "                           df_curr_te[['cell_id', 'TE_id', 'TE_chr','TE_start','TE_end',\n",
        "                                       'TE_rank','region_length','avg_rpm_diff']].rename(columns = {'TE_id' : 'ste_id',\n",
        "                                                                                                    'TE_rank' : 'ste_rank',\n",
        "                                                                                                    'TE_chr' : 'ste_chr',\n",
        "                                                                                                    'TE_start' : 'ste_start',\n",
        "                                                                                                    'TE_end' : 'ste_end'})\n",
        "                          ], ignore_index = True)\n",
        "\n",
        "  SE_table = pd.concat([SE_table, df_curr_se], ignore_index=True)\n",
        "  TE_table = pd.concat([TE_table, df_curr_te], ignore_index=True)\n",
        "  df_ste = pd.concat([df_ste, df_ste_curr], ignore_index=True)"
      ],
      "metadata": {
        "id": "egRq3YvuGBFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing single SE dataset for merge, merging SE and defining consolidated SE loci**"
      ],
      "metadata": {
        "id": "XsQI1lQkTjSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_se_concated = SE_table[['se_chr', 'se_start', 'se_end', 'cell_id', 'se_id', 'se_rank', 'region_length', 'avg_rpm_diff']]\n",
        "df_se_concated[['se_chr', 'se_start', 'se_end', 'cell_id', 'se_id']].to_csv(f'{path}/Intermediate_tables/Set2__SE_concatinated.bed', index = False, sep = '\\t', header = None)"
      ],
      "metadata": {
        "id": "1w3LDUMzGfdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sort -k1,1 -k2,2n {path}/Intermediate_tables/Set2__SE_concatinated.bed >  {path}/Intermediate_tables/sorted.Set2__SE_concatinated.bed\n",
        "!bedtools merge -i {path}/Intermediate_tables/sorted.Set2__SE_concatinated.bed -c 4,4,5,5 -o count_distinct,collapse,count_distinct,collapse -d 12500 > {path}/Intermediate_tables/merged.sorted.Set2__SE_concatinated.bed"
      ],
      "metadata": {
        "id": "nQIBede0ugjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.read_csv(f'{path}/Intermediate_tables/merged.sorted.Set2__SE_concatinated.bed',\n",
        "                        sep = '\\t',\n",
        "                        header = None,\n",
        "                        names = ['se_chr', 'se_start', 'se_end', 'cell_id_count', 'cell_id_list', 'se_id_count',  'se_id_list'])\n",
        "\n",
        "\n",
        "for s in samples:\n",
        "  df_merged[f'SE_{s}'] = df_merged.cell_id_list.apply(lambda x: f'SE_{s}' in x).astype('int')\n",
        "\n",
        "df_merged['se_locus_id'] =  ['se_region_' + str(i+1) for i in range(df_merged.shape[0])]\n",
        "df_merged = df_merged[['se_locus_id', 'se_chr', 'se_start', 'se_end', 'se_id_list',\n",
        "                        'SE_11_0077', 'SE_12_0118',\n",
        "                        'SE_12_0449', 'SE_12_0450']].rename(columns = {'se_chr' : 'chr', 'se_start' : 'start', 'se_end' : 'end'})\n",
        "df_merged.to_csv(f'{path}/Tables/Set2__Table1.csv', index = False)"
      ],
      "metadata": {
        "id": "Hw6kR5Aet9ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head(5)"
      ],
      "metadata": {
        "id": "wQ7JPV1PWBsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intersecting consolidated SE loci with unioned SE+TE datasets**"
      ],
      "metadata": {
        "id": "hjTTWBoha2KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged[['chr', 'start', 'end', 'se_locus_id']].to_csv(f'{path}/Intermediate_tables/Set2__consolidated_SE_loci.bed', sep = '\\t', index = False, header = False)"
      ],
      "metadata": {
        "id": "Mc7ZXT-Gt92L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving unioned SE and TE datasets\n",
        "df_ste[['ste_chr', 'ste_start', 'ste_end',\n",
        "        'cell_id', 'ste_id',  'ste_rank',\n",
        "        'region_length', 'avg_rpm_diff']].to_csv(f'{path}/Intermediate_tables/Set2__SE_TE_concatinated.bed',\n",
        "                                                   sep = '\\t', index = False, header = False)"
      ],
      "metadata": {
        "id": "D1TR8LJnMOAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sort -k1,1 -k2,2n {path}/Intermediate_tables/Set2__SE_TE_concatinated.bed >  {path}/Intermediate_tables/sorted.Set2__SE_TE_concatinated.bed"
      ],
      "metadata": {
        "id": "quQgWqviPtGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bedtools intersect -a {path}/Intermediate_tables/sorted.Set2__SE_TE_concatinated.bed -b {path}/Intermediate_tables/Set2__consolidated_SE_loci.bed -wo -f 0.5 -F 1 -e > {path}/Intermediate_tables/Set2__STE_within_consolidated_SE_loci.bed"
      ],
      "metadata": {
        "id": "8xgvIiSKyBVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing a table of all enhancers signals, both SE and TE, within consolidated SE loci**"
      ],
      "metadata": {
        "id": "lC8GbNmkJDxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ste_within_locus = pd.read_csv(f'{path}/Intermediate_tables/Set2__STE_within_consolidated_SE_loci.bed', sep = '\\t', header = None,\n",
        "                              names = ['ste_chr', 'ste_start', 'ste_end', 'cell_id',\n",
        "                                       'ste_id',  'ste_rank', 'region_length', 'avg_rpm_diff',\n",
        "                                       'chr', 'start', 'end', 'se_locus_id', 'overlap'] )\n",
        "\n",
        "df_ste_within_locus = df_ste_within_locus[['se_locus_id', 'cell_id', 'ste_id', 'ste_chr', 'ste_start', 'ste_end',\n",
        "                    'ste_rank', 'region_length', 'avg_rpm_diff', 'overlap']].merge(df_ste_within_locus.groupby(['se_locus_id', 'cell_id'],\n",
        "                                                                                                               as_index = False)\\\n",
        "                                                                                                       .agg(locus_ste_overlap_total = ('overlap', np.sum)),\n",
        "                                                                                   on = ['se_locus_id', 'cell_id'])\n",
        "df_ste_within_locus['ste_weight_within_locus'] = df_ste_within_locus.overlap/df_ste_within_locus.locus_ste_overlap_total"
      ],
      "metadata": {
        "id": "omVW87f4N8ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ste_within_locus = df_ste_within_locus[['se_locus_id', 'cell_id', 'ste_id',\n",
        "                    'ste_chr', 'ste_start', 'ste_end', 'ste_rank',\n",
        "                     'avg_rpm_diff', 'overlap', 'ste_weight_within_locus']]\n",
        "\n",
        "df_ste_within_locus.to_csv(f'{path}/Tables/Set2__Table2.csv', index = False)"
      ],
      "metadata": {
        "id": "pIczOkJcT3Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining peak (maximum) and weighted average activity within all consolidated SE locus for each sample separately**"
      ],
      "metadata": {
        "id": "CQIYEYpRVZqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ste_within_locus['is_SE'] = df_ste_within_locus.ste_id.str.contains('SE').astype('int')\n",
        "df_ste_within_locus['weighted_avg_rpm_diff'] = df_ste_within_locus.avg_rpm_diff * df_ste_within_locus.ste_weight_within_locus\n",
        "df_locus_activity = df_merged[['se_locus_id']].merge(df_ste_within_locus.groupby(['se_locus_id', 'cell_id'], as_index = False)\\\n",
        "                                                                         .agg(avg_rpm_diff__max = ('avg_rpm_diff', 'max'),\n",
        "                                                                              avg_rpm_diff__weighted = ('weighted_avg_rpm_diff', sum ),\n",
        "                                                                              max_rank = ('ste_rank', 'max'),\n",
        "                                                                              min_rank = ('ste_rank', 'min'),\n",
        "                                                                              active_SE = ('is_SE', 'max'),\n",
        "                                                                              active_SE_count = ('is_SE',  lambda x : np.sum( x) ),\n",
        "                                                                              active_TE_count = ('is_SE', lambda x : np.sum(1 - x) )),\n",
        "                                                       on = 'se_locus_id')"
      ],
      "metadata": {
        "id": "zdiah8BFP5vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_locus_activity.to_csv(f'{path}/Tables/Set2__Table3.csv', index = False)"
      ],
      "metadata": {
        "id": "g81DVQiPZQ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Matrix Construction**"
      ],
      "metadata": {
        "id": "B4VEIrhxbP8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = pd.pivot_table(df_locus_activity, index = 'se_locus_id', columns = 'cell_id', values = 'avg_rpm_diff__max')\n",
        "\n",
        "#sort loci\n",
        "df_features['se_locus_num'] =  df_features.index.str.split('se_region_').str.get(1).astype('int')\n",
        "df_features = df_features.sort_values('se_locus_num')\n",
        "df_features.drop(columns='se_locus_num', inplace = True)"
      ],
      "metadata": {
        "id": "C1Klbex54tDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_signal_max = df_locus_activity[['se_locus_id','cell_id','avg_rpm_diff__max']].copy()\n",
        "df_signal_max['consolidated_SE_locus_signal'] = df_signal_max.avg_rpm_diff__max\n",
        "df_signal_max['method'] = 'peak_activity'\n",
        "\n",
        "df_signal_weighted = df_locus_activity[['se_locus_id','cell_id','avg_rpm_diff__weighted']].copy()\n",
        "df_signal_weighted['consolidated_SE_locus_signal'] = df_signal_weighted.avg_rpm_diff__weighted\n",
        "df_signal_weighted['method'] = 'weighted_avg_activity'\n",
        "\n",
        "df_signal = pd.concat([df_signal_max, df_signal_weighted], ignore_index = True)"
      ],
      "metadata": {
        "id": "-haHtEGMhIUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visually, the difference in signal distribution between the two approaches is not noticeable.\n",
        "# However, both of these approaches are required for deep analysis of a specific locus activity\n",
        "sns.boxplot(df_signal, y='cell_id', x='consolidated_SE_locus_signal', hue = 'method')"
      ],
      "metadata": {
        "id": "xclkJMXv6Jsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I7N3NW05j5fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df_features.isnull()).sum()/df_features.shape[0]"
      ],
      "metadata": {
        "id": "uPBRYiMPj16x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ste_within_locus[df_ste_within_locus.is_SE == 1].groupby('cell_id', as_index = False).agg(se_num = ('ste_rank', 'max'))"
      ],
      "metadata": {
        "id": "BWPCcEf1fTKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature processing**"
      ],
      "metadata": {
        "id": "Qdu3Rp0jkA10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_features.columns:\n",
        "  df_features[f'{col}_medianNormalized'] = df_features[col]/df_features[df_features[col].notnull()][col].median()"
      ],
      "metadata": {
        "id": "YJTY2Kt67TfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Global **data imputation** with a small positive value"
      ],
      "metadata": {
        "id": "JEeBhDR5k77Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_value = 1e-06  #\n",
        "X_medianNormalized_imputed = np.nan_to_num(df_features.iloc[:, 4:].values, nan=imputation_value)\n",
        "df_features[df_features.columns[4:] + '_imputed'] = X_medianNormalized_imputed.copy()"
      ],
      "metadata": {
        "id": "111PDtwtp8K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Log(1+X) data transformation"
      ],
      "metadata": {
        "id": "IXHoqlrTqsuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_log1p = np.log1p(df_features.iloc[:, 8:].values)\n",
        "df_features[df_features.columns[8:] + '_log1p'] = X_log1p.copy()"
      ],
      "metadata": {
        "id": "lFuGf-I3qfJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- z-scaling"
      ],
      "metadata": {
        "id": "h2OtUxT_zsuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ss = StandardScaler()\n",
        "ss.fit(X_log1p)\n",
        "\n",
        "X_zscaled = ss.transform(X_log1p)\n",
        "df_features[df_features.columns[12:] + '_zscaled'] = X_zscaled.copy()"
      ],
      "metadata": {
        "id": "FnOMJRsxzwZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "tXBGe2sbg0UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = df_features.merge(df_merged[['se_locus_id', 'SE_11_0077', 'SE_12_0118',\n",
        "                                           'SE_12_0449', 'SE_12_0450']] ,\n",
        "                   on = 'se_locus_id', suffixes = ('', '_is'))"
      ],
      "metadata": {
        "id": "q8yB_FCbg_57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns = list(df_features.columns)"
      ],
      "metadata": {
        "id": "WTXgOrfOiXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns_ordered = feature_columns[0:1] + feature_columns[21:] + feature_columns[1:21]"
      ],
      "metadata": {
        "id": "JJlkJlkXiEaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features[feature_columns_ordered].to_csv(f'{path}/Tables/Set2__Table4.csv', index = False)"
      ],
      "metadata": {
        "id": "Jhj1p2-th3gQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}